{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviations are tuples, first index of deviation must always specify player (at least for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseCorrelatedEquilibriumProblem:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 player_action_dims,\n",
    "                 observed_strategy,\n",
    "                 payoff_features,\n",
    "                 deviations_dim,\n",
    "                 get_deviation_iter,\n",
    "                 apply_deviation):\n",
    "        self.num_players = len(player_action_dims)\n",
    "        self.player_action_dims = player_action_dims\n",
    "        self.observed_strategy = observed_strategy\n",
    "        self.payoff_features_fn = payoff_features\n",
    "        self.deviations_dim = deviations_dim\n",
    "        self.get_deviation_iter = get_deviation_iter\n",
    "        self.apply_deviation_fn = apply_deviation\n",
    "        self.memoize_regrets_dict = {}\n",
    "        assert self.deviations_dim[0] == self.num_players\n",
    "        self.K = K\n",
    "    \n",
    "    def enumerate_joint_actions(self):\n",
    "        return itertools.product(*[range(d) for d in self.player_action_dims])\n",
    "    \n",
    "    def predicted_strategy(self, theta):\n",
    "        unnormalized_dist = torch.zeros(*self.player_action_dims)\n",
    "        # dot product of each regret feat with each theta\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            action_regret_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            action_regret_scalars = torch.sum(action_regret_feats * theta, dim=len(theta.shape)-1)\n",
    "            unnormalized_dist[joint_action] = torch.exp(-torch.sum(action_regret_scalars))\n",
    "        Z = torch.sum(unnormalized_dist)\n",
    "        return unnormalized_dist / Z\n",
    "\n",
    "    def compute_phi_regrets_for_action(self, action_tens):\n",
    "        key = tuple(action_tens.numpy())\n",
    "        if key in self.memoize_regrets_dict:\n",
    "            return self.memoize_regrets_dict[key]\n",
    "        \n",
    "        \n",
    "    # these are the instantaneous regrets for all the specific deviations\n",
    "        regret_feats = torch.zeros(*self.deviations_dim, self.K, requires_grad=False)\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            deviation_applied = self.apply_deviation_fn(action_tens, deviation)\n",
    "            # get regrets for specific player only (player is specified by 0 of deviation)\n",
    "            regret_feats[deviation] = self.payoff_features_fn(deviation_applied)[deviation[0]] - self.payoff_features_fn(action_tens)[deviation[0]]\n",
    "        self.memoize_regrets_dict[key] = regret_feats\n",
    "        return regret_feats\n",
    "    \n",
    "    def compute_expected_regret_feats(self, action_dist):\n",
    "        total_regret_feats = torch.zeros(*self.deviations_dim, self.K, requires_grad=False)\n",
    "        n = 0\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            n += 1\n",
    "            total_regret_feats += action_dist[joint_action] * self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "        return total_regret_feats / n\n",
    "    \n",
    "    \n",
    "    def analytic_gradient(self, theta):\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        regret_feats_observed = self.compute_expected_regret_feats(self.observed_strategy)\n",
    "        regret_feats_predicted = self.compute_expected_regret_feats(self.predicted_strategy(theta))\n",
    "        g = torch.zeros_like(theta, requires_grad=False)\n",
    "        for deviation in dev_iter():\n",
    "            this_deviation_theta = theta[deviation].view(*[1 for _ in deviation],-1) # unsqueeze to broadcast\n",
    "            # sorry that is a really hacky way to do it, but i think it does what we want\n",
    "            # i.e. add one empty dim for all dims of deviations, then -1 for the dim that is size K\n",
    "            little_scalar_regrets = torch.sum(regret_feats_observed * this_deviation_theta, dim=len(theta.shape)-1)\n",
    "            #  now argmax\n",
    "            \n",
    "            fstar_ind = little_scalar_regrets.argmax()\n",
    "            \n",
    "            g[deviation] = regret_feats_observed.view(-1,self.K)[fstar_ind] - regret_feats_predicted[deviation]\n",
    "        return g\n",
    "            \n",
    "\n",
    "    def maxent_dual_objective(self, theta):\n",
    "        bigZ = torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "        # for each joint action in A\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            little_r_a_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            # scalar features for all deviations f with their own theta_fs\n",
    "            little_r_a_scalar = torch.sum(little_r_a_feats * theta, dim=len(theta.shape)-1)\n",
    "            # sum up, exp, add to Z\n",
    "            bigZ = bigZ + torch.exp( -torch.sum(little_r_a_scalar))\n",
    "        obj = torch.log(bigZ)\n",
    "        # computing expected big regret for theta_f is max over phi_f of r_f(predicted | theta_f)\n",
    "        # phi_f here is just the whole phi\n",
    "        expected_er_feats = self.compute_expected_regret_feats(self.observed_strategy)\n",
    "\n",
    "        # for each deviation\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            this_deviation_theta = theta[deviation].view(*[1 for _ in deviation],-1) # unsqueeze to broadcast\n",
    "            # sorry that is a really hacky way to do it, but i think it does what we want\n",
    "            # i.e. add one empty dim for all dims of deviations, then -1 for the dim that is size K\n",
    "            little_scalar_regrets = torch.sum(expected_er_feats * this_deviation_theta, dim=len(theta.shape)-1)\n",
    "            # little_scalar_regrets contains the regret for theta_f for all the different fs\n",
    "            big_Regret = torch.max(little_scalar_regrets)\n",
    "            obj = obj + big_Regret\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                yield (i, j)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                for k in range(player_action_dims[i]):\n",
    "                    yield (i, j, k)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_external_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, action = deviation\n",
    "    new_action_tens[player] = action\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_switch_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, actionx, actiony = deviation\n",
    "    if new_action_tens[player] == actionx:\n",
    "        new_action_tens[player] = actiony\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_analytic(prob_obj, theta, epochs=100, lr=0.1):\n",
    "    for i in range(epochs):\n",
    "        g = prob_obj.analytic_gradient(theta)\n",
    "        theta -= lr*g\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chicken_feats(action_tuple):\n",
    "    p1, p2 = action_tuple\n",
    "    # 0 is drive, 1 is swerve\n",
    "    # for utility vectors first dim is crash, second dim is look cool, third dim is look like a wimp\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0,0.0], [0.0,0.0,1.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0,1.0], [0.0,1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0,1.0], [0.0,0.0,1.0]])\n",
    "        \n",
    "# explicit payoffs for util vector [-5.0, 1.0, 0.0], for reference\n",
    "chicken_payoffs = torch.tensor([\n",
    "    [[-5.0, 1.0],[0.0,0.0]],\n",
    "    [[-5.0, 0.0],[1.0,0.0]]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pure nash equilibrium\n",
    "\n",
    "for some reason, the output trying to recover a pure Nash equilibrium by using external deviations doesn't work. Admittedly a pure Nash EQ is very low entropy, but the result seems to be a correlated equilibrium and not a Nash equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_nash_chicken = torch.tensor([[0.0,1.0],[0.0,0.0]])\n",
    "pure_nash_chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_ext = InverseCorrelatedEquilibriumProblem(3, (2,2), pure_nash_chicken, chicken_feats, (2,2), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_theta =  torch.zeros(2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5921,  1.1846, -0.5925],\n",
       "         [-1.1846,  0.5921,  0.5925]],\n",
       "\n",
       "        [[-0.5921,  1.1846, -0.5925],\n",
       "         [-1.1846,  0.5921,  0.5925]]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_ext,  chicken_theta, epochs=10000,  lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0139, 0.4861],\n",
       "        [0.4861, 0.0139]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixed Nash equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nash_chicken = torch.tensor([0.1667,.8333]).view(-1,1) @ torch.tensor([.1667, .8333]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0278, 0.1389],\n",
       "        [0.1389, 0.6944]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_nash_chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_ext = InverseCorrelatedEquilibriumProblem(3, (2,2), mixed_nash_chicken, chicken_feats, (2,2), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_analytic =  torch.zeros(2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3250, -0.1992, -0.1258],\n",
       "         [-0.6867, -0.1625,  0.8492]],\n",
       "\n",
       "        [[ 0.3250, -0.1992, -0.1258],\n",
       "         [-0.6867, -0.1625,  0.8492]]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_ext,  chicken_analytic, epochs=10000,  lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0278, 0.1389],\n",
       "        [0.1389, 0.6944]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_analytic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlated equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is another correlated equilibrium per John's slides. sometimes both people are cowardly in this one.\n",
    "corr_chicken = torch.tensor([[0.0,0.4],[0.4,0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "correq_analytic  = torch.zeros(2,2,2,3, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_int = InverseCorrelatedEquilibriumProblem(3, (2,2), corr_chicken, chicken_feats, (2,2,2), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.8500,  0.8394,  1.0106]],\n",
       "\n",
       "         [[-0.2194,  0.2888, -0.0694],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.8500,  0.8394,  1.0106]],\n",
       "\n",
       "         [[-0.2194,  0.2888, -0.0694],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_int, correq_analytic,  epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0013, 0.3996],\n",
       "        [0.3996, 0.1994]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a social-welfare-maximizing correlated equilibrium. Computed by Kevin's code, or it just makes sense.\n",
    "corr_chicken = torch.tensor([[0.0,0.5], [0.5,0.0]])\n",
    "corr_chicken_approx = torch.tensor([[0.0,0.46], [0.54,0.0]])\n",
    "\n",
    "correq_theta = torch.ones(2,2,2,3)\n",
    "chicken_obj_int = InverseCorrelatedEquilibriumProblem(3, (2,2), corr_chicken_approx, chicken_feats, (2,2,2), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,  1.0000,  1.0000],\n",
       "          [-0.9752,  1.9861,  1.9886]],\n",
       "\n",
       "         [[ 0.0139,  2.9751,  0.0110],\n",
       "          [ 1.0000,  1.0000,  1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000,  1.0000,  1.0000],\n",
       "          [-0.9752,  1.9861,  1.9886]],\n",
       "\n",
       "         [[ 0.0139,  2.9751,  0.0110],\n",
       "          [ 1.0000,  1.0000,  1.0000]]]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_int,  correq_theta, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0013, 0.4987],\n",
       "        [0.4987, 0.0013]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
