{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables in program:\n",
    "\n",
    "\n",
    "1. empirical observed joint action probabilities. equiv. to computed joint action probabilities. should be of size (p1_action x p2_action x p3_action ...)\n",
    "\n",
    "2. expected regret features per deviation function -- can be computed once for each joint action and deviation function and then averaged over. expectation under either empirical strategy or dual strategy.\n",
    "\n",
    "\n",
    "3. dual variables -- one utility vector per deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try RPS, computing with external regret deviations only\n",
    "K = 2 # dim of util features (going to be 0/1)\n",
    "N = 2 # num players\n",
    "actions_per_player = 3\n",
    "num_external_deviations = N * actions_per_player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rps_feats(action_tuple):\n",
    "    # 0 is rock, 1 is paper, 2 is scissors\n",
    "    p1, p2 = action_tuple\n",
    "    # feat_vecs has shape N, K\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "    elif p1 == 2:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_external_regrets_for_action( action_tens ):\n",
    "    # these are the instantaneous regrets for all the specific deviations\n",
    "    ext_regrets = torch.zeros(N, actions_per_player, K)\n",
    "    # ext_regrets[i, j] is player i, deviation f_i = ->j\n",
    "    for player in range(N):\n",
    "        for action in range(actions_per_player):\n",
    "            \n",
    "            deviation_applied = torch.clone(action_tens)\n",
    "            deviation_applied[player] = action\n",
    "            \n",
    "            ext_regrets[player, action] = rps_feats(deviation_applied)[player] - rps_feats(action_tens)[player]\n",
    "    return ext_regrets\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_regrets = compute_external_regrets_for_action(torch.tensor([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.],\n",
       "         [ 0., -1.],\n",
       "         [ 1., -1.]],\n",
       "\n",
       "        [[ 0., -1.],\n",
       "         [ 0.,  0.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_external_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expected_external_regret_feats( action_dist ):\n",
    "    total_regret_feats = torch.zeros(N, actions_per_player, K)\n",
    "    n = 0\n",
    "    for p1_act in range(action_dist.shape[0]):\n",
    "        for p2_act in range(action_dist.shape[1]):\n",
    "            n += 1\n",
    "            total_regret_feats += action_dist[p1_act, p2_act] * compute_external_regrets_for_action(torch.tensor([p1_act, p2_act]))\n",
    "    return total_regret_feats / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_eq_rps = torch.tensor([1/3 + 0.001,1/3,1/3 - 0.001]).view(-1,1) @ torch.tensor([1/3,1/3 - 0.01,1/3 + 0.01]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_regret_feats = compute_expected_external_regret_feats(nash_eq_rps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_strategy(regret_feats, theta):\n",
    "    # both regret_feats and theta are N x actions_per_player x K\n",
    "    # output should be of size p1_actions x p2_actions\n",
    "    unnormalized_dist = torch.zeros(actions_per_player, actions_per_player)\n",
    "    # dot product of each regret feat with each theta\n",
    "    for p1_action in range(actions_per_player):\n",
    "        for p2_action in range(actions_per_player):\n",
    "            joint_action = torch.tensor([p1_action, p2_action])\n",
    "            action_regret_feats = compute_external_regrets_for_action(joint_action)\n",
    "            action_regret_scalars = torch.sum(action_regret_feats * theta, dim=2)\n",
    "            unnormalized_dist[p1_action, p2_action] = torch.exp(-torch.sum(action_regret_scalars))\n",
    "    Z = torch.sum(unnormalized_dist)\n",
    "    return unnormalized_dist / Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0004, 0.1665, 0.1665],\n",
       "        [0.1665, 0.0004, 0.1665],\n",
       "        [0.1665, 0.1665, 0.0004]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_strategy(expected_regret_feats, torch.ones_like(expected_regret_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxent_ice_gradient(prediction, empirical, theta):\n",
    "    gradients = torch.zeros(N, actions_per_player, K)\n",
    "    predicted_external_regret_feats = compute_expected_external_regret_feats(prediction)\n",
    "    expected_external_regret_feats = compute_expected_external_regret_feats(empirical)\n",
    "    \n",
    "    for player in range(N):\n",
    "        for action in range(actions_per_player):\n",
    "            # external deviation player, ->action\n",
    "            this_deviation_theta = theta[player, action].view(1,1,-1) # unsqueeze to broadcast\n",
    "            this_deviation_scalar_regrets = torch.sum(expected_external_regrets * this_deviation_theta, dim=2)\n",
    "            f_star = torch.argmax()\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = torch.sum(torch.rand(N, actions_per_player, K) * torch.rand(N, actions_per_player, K), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.7183, 0.8648]),\n",
       "indices=tensor([2, 1]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(scalars, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3877, 0.3270, 0.7183],\n",
       "        [0.4011, 0.8648, 0.6257]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what about just explicitly computing the objective, and letting torch handle the gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxent_dual_objective(observed, theta):\n",
    "    \n",
    "    bigZ = torch.tensor(0.0)\n",
    "    \n",
    "    # for each joint action in A\n",
    "    for p1_action in range(actions_per_player):\n",
    "        for p2_action in range(actions_per_player):\n",
    "            \n",
    "            little_r_a_feats = compute_external_regrets_for_action(torch.tensor([p1_action, p2_action]))\n",
    "            # scalar features for all deviations f with their own theta_fs\n",
    "            little_r_a_scalar = torch.sum(little_r_a_feats * theta, dim=2)\n",
    "            # sum up, exp, add to Z\n",
    "            bigZ += torch.exp( - torch.sum(little_r_a_scalar))\n",
    "    obj = torch.log(bigZ)\n",
    "    # computing expected big regret for theta_f is max over phi_f of r_f(predicted | theta_f)\n",
    "    # phi_f here is just the whole phi\n",
    "    expected_er_feats = compute_expected_external_regret_feats(observed)\n",
    "    \n",
    "    # for each deviation\n",
    "    for player in range(N):\n",
    "        for dev_action in range(actions_per_player):\n",
    "            this_deviation_theta = theta[player, dev_action].view(1,1,-1) # unsqueeze to broadcast\n",
    "            little_scalar_regrets = torch.sum(expected_er_feats * this_deviation_theta, dim=2)\n",
    "            # little_scalar_regrets contains the regret for theta_f for all the different fs\n",
    "            big_Regret = torch.max(little_scalar_regrets)\n",
    "            obj += big_Regret\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1972)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxent_dual_objective(nash_eq_rps, expected_regret_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_theta = torch.rand(expected_regret_feats.shape).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9399, 0.3180],\n",
       "         [0.3251, 0.0824],\n",
       "         [0.3891, 0.1169]],\n",
       "\n",
       "        [[0.5737, 0.3316],\n",
       "         [0.9650, 0.1151],\n",
       "         [0.2646, 0.4102]]], requires_grad=True)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(empirical_dist, theta, epochs=100):\n",
    "    optimizer = optim.Adam([theta], lr=0.1)\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = maxent_dual_objective(empirical_dist, theta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-016bc6bace57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_regret_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnash_eq_rps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-c704b4b21900>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(empirical_dist, theta, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxent_dual_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempirical_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theta = torch.rand(expected_regret_feats.shape).requires_grad_(True)\n",
    "optimize(nash_eq_rps, theta)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1113, 0.1116, 0.1110],\n",
       "        [0.1110, 0.1108, 0.1103],\n",
       "        [0.1116, 0.1117, 0.1108]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_strategy(expected_regret_feats, theta.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_nash_eq = torch.tensor([1/3,1/3,1/3]).view(-1,1) @ torch.tensor([1/3,1/3,1/3]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_regret_feats_exact = compute_expected_external_regret_feats(exact_nash_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.rand(expected_regret_feats_exact.shape).requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0161, 0.2234, 0.0312],\n",
       "        [0.0438, 0.0064, 0.1158],\n",
       "        [0.4999, 0.0534, 0.0101]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_strategy(expected_regret_feats_exact, theta.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2228,  0.2000],\n",
      "         [ 0.2579,  0.2672],\n",
      "         [-0.1545, -0.2178]],\n",
      "\n",
      "        [[-0.2853, -0.2037],\n",
      "         [ 0.1581, -0.2872],\n",
      "         [ 0.2470,  0.2444]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimize(exact_nash_eq, theta)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.1109, 0.1107],\n",
       "        [0.1112, 0.1106, 0.1111],\n",
       "        [0.1121, 0.1111, 0.1113]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_strategy(expected_regret_feats_exact, theta.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
