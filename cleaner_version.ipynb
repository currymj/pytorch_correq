{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviations are tuples, first index of deviation must always specify player (at least for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseCorrelatedEquilibriumProblem:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 player_action_dims,\n",
    "                 observed_strategy,\n",
    "                 payoff_features,\n",
    "                 deviations_dim,\n",
    "                 get_deviation_iter,\n",
    "                 apply_deviation):\n",
    "        self.num_players = len(player_action_dims)\n",
    "        self.player_action_dims = player_action_dims\n",
    "        self.observed_strategy = observed_strategy\n",
    "        self.payoff_features_fn = payoff_features\n",
    "        self.deviations_dim = deviations_dim\n",
    "        self.get_deviation_iter = get_deviation_iter\n",
    "        self.apply_deviation_fn = apply_deviation\n",
    "        assert self.deviations_dim[0] == self.num_players\n",
    "        self.K = K\n",
    "    \n",
    "    def enumerate_joint_actions(self):\n",
    "        return itertools.product(*[range(d) for d in self.player_action_dims])\n",
    "    \n",
    "    def predicted_strategy(self, theta):\n",
    "        unnormalized_dist = torch.zeros(*self.player_action_dims)\n",
    "        # dot product of each regret feat with each theta\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            action_regret_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            action_regret_scalars = torch.sum(action_regret_feats * theta, dim=len(theta.shape)-1)\n",
    "            unnormalized_dist[joint_action] = torch.exp(-torch.sum(action_regret_scalars))\n",
    "        Z = torch.sum(unnormalized_dist)\n",
    "        return unnormalized_dist / Z\n",
    "\n",
    "    def compute_phi_regrets_for_action(self, action_tens):\n",
    "    # these are the instantaneous regrets for all the specific deviations\n",
    "        regret_feats = torch.zeros(*self.deviations_dim, self.K)\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            deviation_applied = self.apply_deviation_fn(action_tens, deviation)\n",
    "            # get regrets for specific player only (player is specified by 0 of deviation)\n",
    "            regret_feats[deviation] = self.payoff_features_fn(deviation_applied)[deviation[0]] - self.payoff_features_fn(action_tens)[deviation[0]]\n",
    "        return regret_feats\n",
    "    \n",
    "    def compute_expected_regret_feats(self, action_dist):\n",
    "        # this can probably be run once and cached\n",
    "        total_regret_feats = torch.zeros(*self.deviations_dim, self.K)\n",
    "        n = 0\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            n += 1\n",
    "            total_regret_feats += action_dist[joint_action] * self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "        return total_regret_feats / n\n",
    "    \n",
    "    \n",
    "    def maxent_dual_objective(self, theta, l1_coeff=0.1):\n",
    "        bigZ = torch.tensor(0.0)\n",
    "    \n",
    "        # for each joint action in A\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            little_r_a_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            # scalar features for all deviations f with their own theta_fs\n",
    "            little_r_a_scalar = torch.sum(little_r_a_feats * theta, dim=len(theta.shape)-1)\n",
    "            # sum up, exp, add to Z\n",
    "            bigZ += torch.exp( -torch.sum(little_r_a_scalar))\n",
    "        obj = torch.log(bigZ)\n",
    "        # computing expected big regret for theta_f is max over phi_f of r_f(predicted | theta_f)\n",
    "        # phi_f here is just the whole phi\n",
    "        expected_er_feats = self.compute_expected_regret_feats(self.observed_strategy)\n",
    "\n",
    "        # for each deviation\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            this_deviation_theta = theta[deviation].view(*[1 for _ in deviation],-1) # unsqueeze to broadcast\n",
    "            # sorry that is a really hacky way to do it, but i think it does what we want\n",
    "            # i.e. add one empty dim for all dims of deviations, then -1 for the dim that is size K\n",
    "            little_scalar_regrets = torch.sum(expected_er_feats * this_deviation_theta, dim=len(theta.shape)-1)\n",
    "            # little_scalar_regrets contains the regret for theta_f for all the different fs\n",
    "            big_Regret = torch.max(little_scalar_regrets)\n",
    "            obj += big_Regret\n",
    "        obj += l1_coeff * torch.norm(theta, 1)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rps_feats(action_tuple):\n",
    "    # 0 is rock, 1 is paper, 2 is scissors\n",
    "    p1, p2 = action_tuple\n",
    "    # feat_vecs has shape N, K\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "    elif p1 == 2:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                yield (i, j)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                for k in range(player_action_dims[i]):\n",
    "                    yield (i, j, k)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_external_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, action = deviation\n",
    "    new_action_tens[player] = action\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_switch_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, actionx, actiony = deviation\n",
    "    if new_action_tens[player] == actionx:\n",
    "        new_action_tens[player] = actiony\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_problem(prob_obj, theta, epochs=100, lr=0.1):\n",
    "    optimizer = optim.Adam([theta], lr=lr)\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = prob_obj.maxent_dual_objective(theta)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_eq_rps = torch.tensor([1/3 + 0.001,1/3,1/3 - 0.001]).view(-1,1) @ torch.tensor([1/3,1/3 - 0.01,1/3 + 0.01]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = InverseCorrelatedEquilibriumProblem(2, (3, 3), nash_eq_rps, rps_feats, (2,3), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.],\n",
       "         [ 0., -1.],\n",
       "         [ 1., -1.]],\n",
       "\n",
       "        [[ 0., -1.],\n",
       "         [ 0.,  0.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.compute_phi_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_theta = torch.rand(2,3,2).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0027, 0.2093, 0.1895],\n",
       "        [0.1628, 0.0033, 0.0772],\n",
       "        [0.0880, 0.2656, 0.0016]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.predicted_strategy(ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_problem(my_obj, ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1118, 0.1084, 0.1087],\n",
       "        [0.1144, 0.1125, 0.1103],\n",
       "        [0.1121, 0.1112, 0.1106]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.predicted_strategy(ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_obj = InverseCorrelatedEquilibriumProblem(2, (3,3), nash_eq_rps, rps_feats, (2,3,3), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0., -1.],\n",
       "          [ 1., -1.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0., -1.],\n",
       "          [ 0.,  0.],\n",
       "          [ 1., -1.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]]]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.compute_phi_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_theta = torch.rand(2,3,3,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0192, 0.1568, 0.0960],\n",
       "        [0.2954, 0.0089, 0.0973],\n",
       "        [0.1191, 0.1933, 0.0139]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_problem(switch_obj, int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1068, 0.1145, 0.1099],\n",
       "        [0.1116, 0.1142, 0.1077],\n",
       "        [0.1088, 0.1157, 0.1107]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chicken_feats(action_tuple):\n",
    "    p1, p2 = action_tuple\n",
    "    # 0 is drive, 1 is swerve\n",
    "    # for utility vectors first dim is crash, second dim is look cool, third dim is look like a wimp\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0,0.0], [0.0,0.0,1.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0,1.0], [0.0,1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0,1.0], [0.0,0.0,1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_nash_chicken_mixed = torch.tensor([0.8, 0.2]).view(-1,1) @ torch.tensor([0.8, 0.2]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_ext = InverseCorrelatedEquilibriumProblem(3, (2,2), pure_nash_chicken_mixed, chicken_feats, (2,2), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_theta = torch.zeros(2,2,3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_problem(chicken_obj_ext, chicken_theta, epochs=100, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4430, 0.2023],\n",
       "        [0.2023, 0.1524]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6400, 0.1600],\n",
       "        [0.1600, 0.0400]])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_nash_chicken_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5033,  6.0951, -7.9300],\n",
       "         [-3.4692, -6.3111, 10.0223]],\n",
       "\n",
       "        [[ 3.5033,  6.0951, -7.9300],\n",
       "         [-3.4692, -6.3111, 10.0223]]], requires_grad=True)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_chicken = torch.tensor([[0.2,0.4],[0.4,0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "correq_theta = torch.zeros(2,2,2,3,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_int = InverseCorrelatedEquilibriumProblem(3, (2,2), corr_chicken, chicken_feats, (2,2,2), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_problem(chicken_obj_int, correq_theta, epochs=100, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0185,  0.0220, -0.0309],\n",
       "          [-4.6689, -4.2408,  9.3307]],\n",
       "\n",
       "         [[ 5.4053,  5.5481, -8.6846],\n",
       "          [-0.0185,  0.0220, -0.0309]]],\n",
       "\n",
       "\n",
       "        [[[-0.0185,  0.0220, -0.0309],\n",
       "          [-4.6689, -4.2408,  9.3307]],\n",
       "\n",
       "         [[ 5.4053,  5.5481, -8.6846],\n",
       "          [-0.0185,  0.0220, -0.0309]]]], requires_grad=True)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correq_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2257, 0.3164],\n",
       "        [0.3164, 0.1416]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.4000],\n",
       "        [0.4000, 0.0000]])"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
