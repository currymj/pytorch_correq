{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviations are tuples, first index of deviation must always specify player (at least for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseCorrelatedEquilibriumProblem:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 player_action_dims,\n",
    "                 observed_strategy,\n",
    "                 payoff_features,\n",
    "                 deviations_dim,\n",
    "                 get_deviation_iter,\n",
    "                 apply_deviation):\n",
    "        self.num_players = len(player_action_dims)\n",
    "        self.player_action_dims = player_action_dims\n",
    "        self.observed_strategy = observed_strategy\n",
    "        self.payoff_features_fn = payoff_features\n",
    "        self.deviations_dim = deviations_dim\n",
    "        self.get_deviation_iter = get_deviation_iter\n",
    "        self.apply_deviation_fn = apply_deviation\n",
    "        assert self.deviations_dim[0] == self.num_players\n",
    "        self.K = K\n",
    "    \n",
    "    def enumerate_joint_actions(self):\n",
    "        return itertools.product(*[range(d) for d in self.player_action_dims])\n",
    "    \n",
    "    def predicted_strategy(self, theta):\n",
    "        unnormalized_dist = torch.zeros(*self.player_action_dims)\n",
    "        # dot product of each regret feat with each theta\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            action_regret_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            action_regret_scalars = torch.sum(action_regret_feats * theta, dim=len(theta.shape)-1)\n",
    "            unnormalized_dist[joint_action] = torch.exp(-torch.sum(action_regret_scalars))\n",
    "        Z = torch.sum(unnormalized_dist)\n",
    "        return unnormalized_dist / Z\n",
    "\n",
    "    def compute_phi_regrets_for_action(self, action_tens):\n",
    "    # these are the instantaneous regrets for all the specific deviations\n",
    "        regret_feats = torch.zeros(*self.deviations_dim, self.K)\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            deviation_applied = self.apply_deviation_fn(action_tens, deviation)\n",
    "            # get regrets for specific player only (player is specified by 0 of deviation)\n",
    "            regret_feats[deviation] = self.payoff_features_fn(deviation_applied)[deviation[0]] - self.payoff_features_fn(action_tens)[deviation[0]]\n",
    "        return regret_feats\n",
    "    \n",
    "    def compute_expected_regret_feats(self, action_dist):\n",
    "        # this can probably be run once and cached\n",
    "        total_regret_feats = torch.zeros(*self.deviations_dim, self.K)\n",
    "        n = 0\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            n += 1\n",
    "            total_regret_feats += action_dist[joint_action] * self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "        return total_regret_feats / n\n",
    "    \n",
    "    \n",
    "    def maxent_dual_objective(self, theta):\n",
    "        bigZ = torch.tensor(0.0)\n",
    "    \n",
    "        # for each joint action in A\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "\n",
    "                little_r_a_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "                # scalar features for all deviations f with their own theta_fs\n",
    "                little_r_a_scalar = torch.sum(little_r_a_feats * theta, dim=len(theta.shape)-1)\n",
    "                # sum up, exp, add to Z\n",
    "                bigZ += torch.exp( -torch.sum(little_r_a_scalar))\n",
    "        obj = torch.log(bigZ)\n",
    "        # computing expected big regret for theta_f is max over phi_f of r_f(predicted | theta_f)\n",
    "        # phi_f here is just the whole phi\n",
    "        expected_er_feats = self.compute_expected_regret_feats(self.observed_strategy)\n",
    "\n",
    "        # for each deviation\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            this_deviation_theta = theta[deviation].view(*[1 for _ in deviation],-1) # unsqueeze to broadcast\n",
    "            # sorry that is a really hacky way to do it, but i think it does what we want\n",
    "            # i.e. add one empty dim for all dims of deviations, then -1 for the dim that is size K\n",
    "            little_scalar_regrets = torch.sum(expected_er_feats * this_deviation_theta, dim=len(theta.shape)-1)\n",
    "            # little_scalar_regrets contains the regret for theta_f for all the different fs\n",
    "            big_Regret = torch.max(little_scalar_regrets)\n",
    "            obj += big_Regret\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rps_feats(action_tuple):\n",
    "    # 0 is rock, 1 is paper, 2 is scissors\n",
    "    p1, p2 = action_tuple\n",
    "    # feat_vecs has shape N, K\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "    elif p1 == 2:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                yield (i, j)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                for k in range(player_action_dims[i]):\n",
    "                    yield (i, j, k)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_external_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, action = deviation\n",
    "    new_action_tens[player] = action\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_switch_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, actionx, actiony = deviation\n",
    "    if new_action_tens[player] == actionx:\n",
    "        new_action_tens[player] = actiony\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(prob_obj, theta, epochs=100):\n",
    "    optimizer = optim.Adam([theta], lr=0.1)\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = prob_obj.maxent_dual_objective(theta)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_eq_rps = torch.tensor([1/3 + 0.001,1/3,1/3 - 0.001]).view(-1,1) @ torch.tensor([1/3,1/3 - 0.01,1/3 + 0.01]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = InverseCorrelatedEquilibriumProblem(2, (3, 3), nash_eq_rps, rps_feats, (2,3), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.],\n",
       "         [ 0., -1.],\n",
       "         [ 1., -1.]],\n",
       "\n",
       "        [[ 0., -1.],\n",
       "         [ 0.,  0.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.compute_phi_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_theta = torch.rand(2,3,2).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0074, 0.2031, 0.0657],\n",
       "        [0.0554, 0.0029, 0.1663],\n",
       "        [0.4267, 0.0654, 0.0071]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.predicted_strategy(ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(my_obj, ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1112, 0.1109, 0.1113],\n",
       "        [0.1103, 0.1102, 0.1109],\n",
       "        [0.1119, 0.1112, 0.1121]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.predicted_strategy(ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_obj = InverseCorrelatedEquilibriumProblem(2, (3,3), nash_eq_rps, rps_feats, (2,3,3), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0., -1.],\n",
       "          [ 1., -1.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0., -1.],\n",
       "          [ 0.,  0.],\n",
       "          [ 1., -1.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.compute_phi_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_theta = torch.rand(2,3,3,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0102, 0.1721, 0.1508],\n",
       "        [0.1628, 0.0041, 0.2825],\n",
       "        [0.1304, 0.0829, 0.0041]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(switch_obj, int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1113, 0.1106, 0.1112],\n",
       "        [0.1115, 0.1108, 0.1111],\n",
       "        [0.1115, 0.1110, 0.1108]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
