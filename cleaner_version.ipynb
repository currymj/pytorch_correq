{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deviations are tuples, first index of deviation must always specify player (at least for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseCorrelatedEquilibriumProblem:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 player_action_dims,\n",
    "                 observed_strategy,\n",
    "                 payoff_features,\n",
    "                 deviations_dim,\n",
    "                 get_deviation_iter,\n",
    "                 apply_deviation):\n",
    "        self.num_players = len(player_action_dims)\n",
    "        self.player_action_dims = player_action_dims\n",
    "        self.observed_strategy = observed_strategy\n",
    "        self.payoff_features_fn = payoff_features\n",
    "        self.deviations_dim = deviations_dim\n",
    "        self.get_deviation_iter = get_deviation_iter\n",
    "        self.apply_deviation_fn = apply_deviation\n",
    "        self.memoize_regrets_dict = {}\n",
    "        assert self.deviations_dim[0] == self.num_players\n",
    "        self.K = K\n",
    "    \n",
    "    def enumerate_joint_actions(self):\n",
    "        return itertools.product(*[range(d) for d in self.player_action_dims])\n",
    "    \n",
    "    def predicted_strategy(self, theta):\n",
    "        unnormalized_dist = torch.zeros(*self.player_action_dims)\n",
    "        # dot product of each regret feat with each theta\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            action_regret_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            action_regret_scalars = torch.sum(action_regret_feats * theta, dim=len(theta.shape)-1)\n",
    "            unnormalized_dist[joint_action] = torch.exp(-torch.sum(action_regret_scalars))\n",
    "        Z = torch.sum(unnormalized_dist)\n",
    "        return unnormalized_dist / Z\n",
    "\n",
    "    def compute_phi_regrets_for_action(self, action_tens):\n",
    "        key = tuple(action_tens.numpy())\n",
    "        if key in self.memoize_regrets_dict:\n",
    "            return self.memoize_regrets_dict[key]\n",
    "        \n",
    "        \n",
    "    # these are the instantaneous regrets for all the specific deviations\n",
    "        regret_feats = torch.zeros(*self.deviations_dim, self.K, requires_grad=False)\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            deviation_applied = self.apply_deviation_fn(action_tens, deviation)\n",
    "            # get regrets for specific player only (player is specified by 0 of deviation)\n",
    "            regret_feats[deviation] = self.payoff_features_fn(deviation_applied)[deviation[0]] - self.payoff_features_fn(action_tens)[deviation[0]]\n",
    "        self.memoize_regrets_dict[key] = regret_feats\n",
    "        return regret_feats\n",
    "    \n",
    "    def compute_expected_regret_feats(self, action_dist):\n",
    "        total_regret_feats = torch.zeros(*self.deviations_dim, self.K, requires_grad=False)\n",
    "        n = 0\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            n += 1\n",
    "            total_regret_feats += action_dist[joint_action] * self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "        return total_regret_feats / n\n",
    "    \n",
    "    \n",
    "    def analytic_gradient(self, theta):\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        regret_feats_observed = self.compute_expected_regret_feats(self.observed_strategy)\n",
    "        regret_feats_predicted = self.compute_expected_regret_feats(self.predicted_strategy(theta))\n",
    "        g = torch.zeros_like(theta, requires_grad=False)\n",
    "        for deviation in dev_iter():\n",
    "            this_deviation_theta = theta[deviation].view(*[1 for _ in deviation],-1) # unsqueeze to broadcast\n",
    "            # sorry that is a really hacky way to do it, but i think it does what we want\n",
    "            # i.e. add one empty dim for all dims of deviations, then -1 for the dim that is size K\n",
    "            little_scalar_regrets = torch.sum(regret_feats_observed * this_deviation_theta, dim=len(theta.shape)-1)\n",
    "            #  now argmax\n",
    "            \n",
    "            fstar_ind = little_scalar_regrets.argmax()\n",
    "            \n",
    "            g[deviation] = regret_feats_observed.view(-1,self.K)[fstar_ind] - regret_feats_predicted[deviation]\n",
    "        return g\n",
    "            \n",
    "\n",
    "    def maxent_dual_objective(self, theta):\n",
    "        bigZ = torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "        # for each joint action in A\n",
    "        for joint_action in self.enumerate_joint_actions():\n",
    "            little_r_a_feats = self.compute_phi_regrets_for_action(torch.tensor(list(joint_action)))\n",
    "            # scalar features for all deviations f with their own theta_fs\n",
    "            little_r_a_scalar = torch.sum(little_r_a_feats * theta, dim=len(theta.shape)-1)\n",
    "            # sum up, exp, add to Z\n",
    "            bigZ = bigZ + torch.exp( -torch.sum(little_r_a_scalar))\n",
    "        obj = torch.log(bigZ)\n",
    "        # computing expected big regret for theta_f is max over phi_f of r_f(predicted | theta_f)\n",
    "        # phi_f here is just the whole phi\n",
    "        expected_er_feats = self.compute_expected_regret_feats(self.observed_strategy)\n",
    "\n",
    "        # for each deviation\n",
    "        dev_iter = self.get_deviation_iter(self.player_action_dims)\n",
    "        for deviation in dev_iter():\n",
    "            this_deviation_theta = theta[deviation].view(*[1 for _ in deviation],-1) # unsqueeze to broadcast\n",
    "            # sorry that is a really hacky way to do it, but i think it does what we want\n",
    "            # i.e. add one empty dim for all dims of deviations, then -1 for the dim that is size K\n",
    "            little_scalar_regrets = torch.sum(expected_er_feats * this_deviation_theta, dim=len(theta.shape)-1)\n",
    "            # little_scalar_regrets contains the regret for theta_f for all the different fs\n",
    "            big_Regret = torch.max(little_scalar_regrets)\n",
    "            obj = obj + big_Regret\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rps_feats(action_tuple):\n",
    "    # 0 is rock, 1 is paper, 2 is scissors\n",
    "    p1, p2 = action_tuple\n",
    "    # feat_vecs has shape N, K\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "    elif p1 == 2:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,1.0],[0.0,1.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "        if p2 == 2:\n",
    "            return torch.tensor([[0.0,0.0],[0.0,0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                yield (i, j)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_enumerator(player_action_dims):\n",
    "    def e():\n",
    "        for i in range(len(player_action_dims)):\n",
    "            for j in range(player_action_dims[i]):\n",
    "                for k in range(player_action_dims[i]):\n",
    "                    yield (i, j, k)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_external_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, action = deviation\n",
    "    new_action_tens[player] = action\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_switch_deviation(action_tens, deviation):\n",
    "    new_action_tens = torch.clone(action_tens)\n",
    "    player, actionx, actiony = deviation\n",
    "    if new_action_tens[player] == actionx:\n",
    "        new_action_tens[player] = actiony\n",
    "    return new_action_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_problem(prob_obj, theta, epochs=100, lr=0.1):\n",
    "    optimizer = optim.SGD([theta], lr=lr)\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = prob_obj.maxent_dual_objective(theta)\n",
    "        print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_analytic(prob_obj, theta, epochs=100, lr=0.1):\n",
    "    for i in range(epochs):\n",
    "        g = prob_obj.analytic_gradient(theta)\n",
    "        theta -= lr*g\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_projections(prob_obj, theta, epochs=100, lr=0.1):\n",
    "    optimizer = optim.Adam([theta], lr=lr)\n",
    "    for i in range(epochs):\n",
    "        l2_norms = torch.norm(theta, 2, dim=len(theta.shape)-1)\n",
    "        scalings = torch.min(torch.ones_like(l2_norms), 1.0/l2_norms)\n",
    "        theta.data = theta.data * scalings.unsqueeze(len(theta.shape)-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = prob_obj.maxent_dual_objective(theta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_eq_rps = torch.tensor([1/3 + 0.001,1/3,1/3 - 0.001]).view(-1,1) @ torch.tensor([1/3,1/3 - 0.01,1/3 + 0.01]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1114, 0.1081, 0.1148],\n",
       "        [0.1111, 0.1078, 0.1144],\n",
       "        [0.1108, 0.1075, 0.1141]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nash_eq_rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = InverseCorrelatedEquilibriumProblem(2, (3, 3), nash_eq_rps, rps_feats, (2,3), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.],\n",
       "         [ 0., -1.],\n",
       "         [ 1., -1.]],\n",
       "\n",
       "        [[ 0., -1.],\n",
       "         [ 0.,  0.],\n",
       "         [ 1., -1.]]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.compute_phi_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0113, 0.1564, 0.1413],\n",
       "        [0.3235, 0.0068, 0.0828],\n",
       "        [0.1357, 0.1396, 0.0026]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.predicted_strategy(ext_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_theta = torch.rand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.9537e-05,  1.3165e-04],\n",
       "         [ 9.4938e-06,  5.7676e-05],\n",
       "         [ 6.0048e-05,  2.7983e-06]],\n",
       "\n",
       "        [[-6.5082e-05,  2.8552e-05],\n",
       "         [-9.8819e-05,  7.7228e-05],\n",
       "         [-8.0911e-05,  5.7632e-05]]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(my_obj, analytic_theta, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.1111, 0.1111],\n",
       "        [0.1111, 0.1111, 0.1112],\n",
       "        [0.1112, 0.1111, 0.1111]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_obj.predicted_strategy(analytic_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_obj = InverseCorrelatedEquilibriumProblem(2, (3,3), nash_eq_rps, rps_feats, (2,3,3), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.],\n",
       "          [ 0., -1.],\n",
       "          [ 1., -1.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0., -1.],\n",
       "          [ 0.,  0.],\n",
       "          [ 1., -1.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0.,  0.],\n",
       "          [ 0.,  0.]]]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.compute_phi_regrets_for_action(torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_theta = torch.rand(2,3,3,2, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0074, 0.1662, 0.1147],\n",
       "        [0.2262, 0.0089, 0.0635],\n",
       "        [0.2303, 0.1796, 0.0031]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(analytic_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0746e-01,  1.0376e-01],\n",
       "          [ 2.7552e-05, -2.6473e-05],\n",
       "          [ 5.2302e-02, -2.9471e-02]],\n",
       "\n",
       "         [[ 2.1693e-02, -2.6166e-02],\n",
       "          [ 9.3484e-02,  1.8532e-01],\n",
       "          [-5.3041e-02,  2.2770e-02]],\n",
       "\n",
       "         [[ 5.7437e-02,  9.8091e-02],\n",
       "          [ 2.5953e-02, -2.5751e-02],\n",
       "          [ 2.2412e-01,  4.4416e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.7583e-05,  3.3810e-05],\n",
       "          [ 5.2689e-02,  2.6368e-02],\n",
       "          [ 1.8386e-05, -7.7079e-06]],\n",
       "\n",
       "         [[ 9.4466e-02,  1.8715e-01],\n",
       "          [-2.8446e-05,  5.6285e-05],\n",
       "          [ 1.0382e-01,  1.8895e-02]],\n",
       "\n",
       "         [[-5.2737e-06, -2.2211e-05],\n",
       "          [-2.2360e-02,  2.2373e-02],\n",
       "          [ 2.6455e-01,  1.3234e-01]]]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(switch_obj, analytic_theta, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1114, 0.1134, 0.1119],\n",
       "        [0.1109, 0.1074, 0.1108],\n",
       "        [0.1133, 0.1107, 0.1101]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(analytic_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_theta = torch.rand(2,3,3,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0088, 0.1171, 0.2765],\n",
       "        [0.0904, 0.0144, 0.1407],\n",
       "        [0.1124, 0.2356, 0.0041]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_theta = optimize_problem(switch_obj, int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1111, 0.1123, 0.1110],\n",
       "        [0.1121, 0.1130, 0.1113],\n",
       "        [0.1108, 0.1115, 0.1067]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(int_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0049, 0.1204, 0.2885],\n",
       "        [0.0852, 0.0097, 0.1399],\n",
       "        [0.1066, 0.2432, 0.0016]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_obj.predicted_strategy(avg_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chicken_feats(action_tuple):\n",
    "    p1, p2 = action_tuple\n",
    "    # 0 is drive, 1 is swerve\n",
    "    # for utility vectors first dim is crash, second dim is look cool, third dim is look like a wimp\n",
    "    if p1 == 0:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[1.0,0.0,0.0], [1.0,0.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,1.0,0.0], [0.0,0.0,1.0]])\n",
    "    elif p1 == 1:\n",
    "        if p2 == 0:\n",
    "            return torch.tensor([[0.0,0.0,1.0], [0.0,1.0,0.0]])\n",
    "        if p2 == 1:\n",
    "            return torch.tensor([[0.0,0.0,1.0], [0.0,0.0,1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pure nash equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit payoffs for util vector [-5.0, 1.0, 0.0]\n",
    "chicken_payoffs = torch.tensor([\n",
    "    [[-5.0, 1.0],[0.0,0.0]],\n",
    "    [[-5.0, 0.0],[1.0,0.0]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_nash_chicken = torch.tensor([0.1667,.8333]).view(-1,1) @ torch.tensor([.1667, .8333]).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0278, 0.1389],\n",
       "        [0.1389, 0.6944]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_nash_chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_ext = InverseCorrelatedEquilibriumProblem(3, (2,2), mixed_nash_chicken, chicken_feats, (2,2), external_enumerator, apply_external_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_analytic =  torch.zeros(2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3303, -0.2071, -0.1233],\n",
       "         [-0.6548, -0.1720,  0.8267]],\n",
       "\n",
       "        [[ 0.3303, -0.2071, -0.1233],\n",
       "         [-0.6548, -0.1720,  0.8267]]])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_ext,  chicken_analytic, epochs=1000,  lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0303, 0.1375],\n",
       "        [0.1375, 0.6947]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_theta = torch.zeros(2,2,3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_problem(chicken_obj_ext, chicken_theta, epochs=1000, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1389, 0.1945],\n",
       "        [0.1945, 0.4722]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_ext.predicted_strategy(chicken_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlated equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is another correlated equilibrium per John's slides. sometimes both people are cowardly in this one.\n",
    "corr_chicken = torch.tensor([[0.0,0.4],[0.4,0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "correq_theta = torch.zeros(2,2,2,3,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "correq_analytic  = torch.zeros(2,2,2,3, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_obj_int = InverseCorrelatedEquilibriumProblem(3, (2,2), corr_chicken, chicken_feats, (2,2,2), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.8815,  0.8529,  1.0287]],\n",
       "\n",
       "         [[-0.2229,  0.2892, -0.0663],\n",
       "          [ 0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [-1.8815,  0.8529,  1.0287]],\n",
       "\n",
       "         [[-0.2229,  0.2892, -0.0663],\n",
       "          [ 0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_int, correq_analytic,  epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0012, 0.3994],\n",
       "        [0.3994, 0.2000]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_analytic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.0000e-02,  4.0000e-02,  8.1956e-10],\n",
       "          [-6.4989e+00, -6.1021e+00,  1.2601e+01]],\n",
       "\n",
       "         [[ 6.0921e+00,  6.3569e+00, -1.2449e+01],\n",
       "          [-4.0000e-02,  4.0000e-02,  8.1956e-10]]],\n",
       "\n",
       "\n",
       "        [[[-4.0000e-02,  4.0000e-02,  8.1956e-10],\n",
       "          [-6.4989e+00, -6.1021e+00,  1.2601e+01]],\n",
       "\n",
       "         [[ 6.0921e+00,  6.3569e+00, -1.2449e+01],\n",
       "          [-4.0000e-02,  4.0000e-02, -1.8626e-09]]]], requires_grad=True)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correq_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1250, 0.3250],\n",
       "        [0.3250, 0.2250]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a social-welfare-maximizing correlated equilibrium. Computed by Kevin's code, or it just makes sense.\n",
    "corr_chicken = torch.tensor([[0.0,0.5], [0.5,0.0]])\n",
    "corr_chicken_approx = torch.tensor([[0.0,0.46], [0.54,0.0]])\n",
    "\n",
    "correq_theta = torch.ones(2,2,2,3)\n",
    "chicken_obj_int = InverseCorrelatedEquilibriumProblem(3, (2,2), corr_chicken, chicken_feats, (2,2,2), switch_enumerator, apply_switch_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500],\n",
       "        [0.2500, 0.2500]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "          [-9.7536e-01,  1.9754e+00,  2.0000e+00]],\n",
       "\n",
       "         [[ 2.4636e-02,  2.9754e+00,  1.5948e-06],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
       "          [-9.7536e-01,  1.9754e+00,  2.0000e+00]],\n",
       "\n",
       "         [[ 2.4636e-02,  2.9754e+00,  1.5948e-06],\n",
       "          [ 1.0000e+00,  1.0000e+00,  1.0000e+00]]]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_analytic(chicken_obj_int,  correq_theta, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0014, 0.4986],\n",
       "        [0.4986, 0.0014]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1250, 0.3750],\n",
       "        [0.3750, 0.1250]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicken_obj_int.predicted_strategy(correq_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9931, 1.0069, 1.0000], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correq_theta[0,0,1]\n",
    "correq_theta[0,1,0]\n",
    "correq_theta[1,0,1]\n",
    "correq_theta[1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
